# -*- coding: utf-8 -*-
"""predictive-analysis-hospital-readmission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qOV55UghZKMAL1H3w-th8FI-vPpq-idu

# Proyek Machine Learning - Prediction of Hospital Readmission

Nama: Farrel Jonathan Vickeldo

Email: farrel.jonathan.fj@gmail.com

ID Dicoding: falthackel

## Business Understanding

### Problem Statement

1. Tingginya angka hospital readmission pada pasien diabetes mencerminkan permasalahan kualitas perawatan rawat inap, khususnya di luar ICU.
2. Implementasi protokol berbasis data, seperti pengukuran HbA1c, memiliki potensi untuk mengurangi angka readmission dan meningkatkan efisiensi biaya kesehatan.
3. Kurangnya evaluasi nasional mengenai perawatan diabetes di rumah sakit menjadi hambatan dalam menciptakan panduan dan baseline yang relevan untuk perubahan sistem perawatan.
4. Tantangan teknis dalam analisis data klinis yang kompleks dan heterogen perlu diatasi untuk menggali wawasan baru yang dapat meningkatkan keselamatan pasien dan efisiensi sistem kesehatan.

### Goals

1.   Faktor-faktor apa yang menjadi prediktor terkuat untuk rawat inap ulang pasien diabetes?
2.   Seberapa baik kita dapat memprediksi rawat inap ulang pasien diabetes dalam kumpulan data ini dengan fitur terbatas?

## Data Understanding

### Variabel-variabel dalam dataset

1. `encounter_id` id unik untuk setiap pertemuan
2. `patient_nbr` id unik untuk setiap pasien
3. `race` nilai: Caucasian, Asian, African American, Hispanic, and lainnya
4. `gender` nilai: male, female, dan unknown/invalid
5. `age` dikelompokkan dalam interval 10 tahun: [0, 10), [10, 20),..., [90, 100)
6. `weight` Weight dalam pon.
7. `admission_type_id` id integer yang sesuai dengan 9 nilai berbeda, misalnya, emergency, urgent, elective, newborn, dan not available.
8. `discharge_disposition_id` id identifier yang sesuai dengan 29 nilai yang berbeda, seperti discharged to home, expired, and not available.
9. `admission_source_id` id identifier yang sesuai dengan 21 nilai yang berbeda, seperti physician referral, emergency room, and transfer from a hospital.
10. `time_in_hospital` jumlah hari antara penerimaan dan pemulangan.
11. `payer_code` id integer yang sesuai dengan 23 nilai yang berbeda, seperti Blue Cross/Blue Shield, Medicare, and self-pay Medical
12. `medical_specialty` Integer identifier of a specialty of the admitting physician, corresponding to 84 distinct values, for example, cardiology, internal medicine, family/general practice, and surgeon.
13. `num_lab_procedures` jumlah tes laboratorium yang dilakukan selama pertemuan.
14. `num_procedures` jumlah prosedur (selain tes laboratorium) yang dilakukan selama pertemuan.
15. `num_medications` jumlah nama generik berbeda yang diberikan selama pertemuan.
16. `number_outpatient` jumlah kunjungan pasien rawat jalan pada tahun sebelum pertemuan.
17. `number_emergency` jumlah kunjungan gawat darurat pasien pada tahun sebelum pertemuan.
18. `number_inpatient` jumlah kunjungan pasien rawat inap pada tahun sebelum pertemuan.
19. `diag_1` diagnosis utama (dikodekan sebagai tiga digit pertama ICD9); 848 nilai berbeda.
20. `diag_2` diagnosis sekunder (dikodekan sebagai tiga digit pertama ICD9); 923 nilai berbeda.
21. `diag_3` diagnosis sekunder tambahan (dikodekan sebagai tiga digit pertama ICD9); 954 nilai berbeda.
22. `number_diagnoses` jumlah diagnosis yang dimasukkan ke dalam sistem.
23. `max_glu_serum` menunjukkan kisaran hasil atau jika tes tidak dilakukan. Nilai: >200, >300, normal, dan tidak ada jika tidak diukur.
24. `A1Cresult` menunjukkan rentang hasil atau jika tes tidak dilakukan. Nilai: >8 jika hasilnya lebih besar dari 8%, >7 jika hasilnya lebih besar dari 7% tetapi kurang dari 8%, normal jika hasilnya kurang dari 7%, dan tidak ada jika tidak diukur.
25. `change` Menunjukkan jika ada perubahan pada obat diabetes (baik dosis atau nama generik). Nilai: change dan no change.
26. `diabetesMed` menunjukkan apakah ada obat diabetes yang diresepkan. Nilai: yes dan no.
27. 24 fitur untuk obat-obatan Untuk nama generik: `metformin`, `repaglinide`, `nateglinide`, `chlorpropamide`, `glimepiride`, `acetohexamide`, `glipizide`, `glyburide`, `tolbutamide`, `pioglitazone`, `rosiglitazone`, `acarbose`, `miglitol`, `troglitazone`, `tolazamide`, `examide`, `sitagliptin`, `insulin`, `glyburide-metformin`, `glipizide-metformin`, `glimepiride- pioglitazone`, `metformin-rosiglitazone`, dan `metformin-pioglitazone`. Fitur ini menunjukkan apakah obat tersebut diresepkan atau ada perubahan dosis. Nilai: “naik” jika dosis ditingkatkan selama pertemuan, “turun” jika dosis dikurangi, “stabil” jika dosis tidak berubah, dan “tidak” jika obat tidak diresepkan.
28. `readmitted` jumlah hari hingga pasien rawat inap kembali. Nilai: <30 jika pasien dirawat kembali dalam waktu kurang dari 30 hari, >30 jika pasien dirawat kembali dalam waktu lebih dari 30 hari, dan Tidak untuk tidak ada catatan pasien yang dirawat kembali.

## Step 1: Import Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from imblearn.over_sampling import SMOTE
from statsmodels.stats.outliers_influence import variance_inflation_factor

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PowerTransformer
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

from imblearn.over_sampling import SMOTE
from collections import Counter

"""## Step 2: Data Preprocessing

### Load Data
"""

file_path = "/content/diabetic_data.csv"
df = pd.read_csv(file_path)

df.head(5).T

"""Jika dilihat diatas, terdapat beberapa data yang kosong dan direpresentasikan dalam `?` dan NaN. Nilai yang kosong ini akan diproses lebih lanjut."""

df.shape

df.info()

for col in df.columns:
    if df[col].isin(['?']).any():  # Check jika ada '?' dalam setiap fitur
        print(f"Nilai '?' dalam kolom {col} adalah {df[col].isin(['?']).sum()}")

"""Dari kode diatas, tercatat ada 7 fitur yang memiliki nilai kosong '?' dan ketujuh fitur tersebut bertipe data object. Akan tetapi, fitur `max_glu_serum` dan `A1Cresult` memiliki nilai NULL yang tidak diisi dengan '?'."""

for col in df.columns:
    if df[col].isna().any():  # Check jika ada 'NaN' dalam kolom
        print(f"Nilai 'NaN' dalam kolom {col} adalah {df[col].isna().sum()}")

"""Kode teratas telah mencatat semua nilai kosong yang dimiliki dalam dataset. Akan tetapi, jika dibaca deskripsi variabel dataset yang digunakan, tidak ada missing value pada kolom `max_glu_serum` dan `AICresult`. Artinya, nilai NaN adalah sebuah kondisi yang disengaja. Oleh karena itu, data ini tidak akan diproses dengan penghilangan."""

for col in df.columns[24:47]:
    print(f"Nilai unik dalam kolom {col} adalah {df[col].unique()}")

"""Meskipun tidak ada nilai NULL, kolom `examide` dan `citoglipton` tidak menunjukkan adanya variasi nilai. Fitur ini tidak akan dapat memberikan informasi yang berarti untuk diproses lebih lanjut. Oleh karena itu, kolom ini akan dihilangkan."""

print(f"Nilai unik dalam kolom gender adalah {df['gender'].unique()}")
print(f"Nilai 'Unknown/Invalid' dalam kolom gender adalah {df['gender'].isin(['Unknown/Invalid']).sum()}")

print(f"Nilai unik dalam kolom race adalah {df['race'].unique()}")
# print(f"Nilai 'Unknown/Invalid' dalam kolom gender adalah {df['gender'].isin(['Unknown/Invalid']).sum()}")

"""### Menangani Nilai yang Hilang

Ada beberapa fitur yang akan dihilangkan, yaitu
1. Variabel `weight` berisi sekitar 98% dari nilai yang hilang.
2. Variabel `payer_code` berisi sekitar 40% nilai yang hilang.
3. Variabel `medical_speciality` berisi sekitar 50% nilai yang hilang.
4. Variabel `examide` dan `citoglipton` karena hanya berisikan nilai No.

Ada beberapa baris yang akan dihilangkan untuk mengatasi nilai kosong pada fitur: `race`, `diag_1`, `diag_2`, `diag_3`, dan `gender`.
"""

# Drop kolom dengan jumlah missing value yang besar
df = df.drop(['weight','payer_code','medical_specialty'], axis = 1)

drop_Idx = set(df[(df['diag_1'] == '?') & (df['diag_2'] == '?') & (df['diag_3'] == '?')].index)

drop_Idx = drop_Idx.union(set(df['diag_1'][df['diag_1'] == '?'].index))
drop_Idx = drop_Idx.union(set(df['diag_2'][df['diag_2'] == '?'].index))
drop_Idx = drop_Idx.union(set(df['diag_3'][df['diag_3'] == '?'].index))

drop_Idx = drop_Idx.union(set(df['race'][df['race'] == '?'].index))
drop_Idx = drop_Idx.union(set(df[df['discharge_disposition_id'] == 11].index))
drop_Idx = drop_Idx.union(set(df['gender'][df['gender'] == 'Unknown/Invalid'].index))

new_Idx = list(set(df.index) - set(drop_Idx))
df = df.iloc[new_Idx]

# Drop kolom `citoglipton` dan `examide`
df = df.drop(['citoglipton', 'examide'], axis = 1)

df.head(5).T

df.shape

"""## Step 3: Data Processing

### Encoding

Fitur `age` memiliki nilai object yang menglasfikasikan umur pasien menjadi kelompok umur dengan rentang 10 tahun. Oleh karena itu, kita akan mengubah setiap rentang menjadi nilai tengah kelompok umur tersebut, mulai dari [0-10) menjadi 5, [10-20) menjadi 10, dst.
"""

# Mengubah kelompok umur menjadi nilai 1-10
for i in range(0,10):
    df['age'] = df['age'].replace('['+str(10*i)+'-'+str(10*(i+1))+')', (i+1)*10 - 5)
df['age'].value_counts()

"""Jumlah perubahan pengobatan: Kumpulan data berisi 23 fitur untuk 23 obat (atau kombinasi) yang menunjukkan untuk masing-masing obat, apakah ada perubahan pengobatan yang dilakukan atau tidak selama pasien dirawat di rumah sakit saat ini. Penelitian sebelumnya menunjukkan bahwa perubahan pengobatan untuk penderita diabetes saat masuk rumah sakit berhubungan dengan tingkat readmisi yang lebih rendah. Tujuannya adalah untuk menyederhanakan model dan mungkin menemukan hubungan dengan jumlah perubahan terlepas dari obat mana yang diubah."""

# Create a list of drug feature column names
drug_features = [
    'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',
    'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone',
    'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide',
    'insulin', 'glyburide-metformin', 'glipizide-metformin',
    'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone'
]

# Create a new column 'numchange' to store the number of drug changes per patient
df['numchange'] = df[drug_features].applymap(lambda x: 0 if x in ['No', 'Steady'] else 1).sum(axis=1)

# Check the distribution of 'numchange' values
df['numchange'].value_counts()

"""Beberapa fitur menggunakan tipe data object, sehingga akan dilakukan encoding.
1. `change`
2. `gender`
3. `diabetesMed`
4. `readmitted`
5. `A1Cresult`
6. `max_glu_serum`

Fitur baru yang dibuat dari encoding fitur lainnya adalah
1. `group_diag_1` dari `diag_1`
2. `group_diag_2` dari `diag_2`
3. `group_diag_3` dari `diag_3`

"""

df['race'] = df['race'].replace({'Caucasian': 1, 'AfricanAmerican': 2, 'Asian': 3, 'Hispanic': 4, 'Other': 5})
df['change'] = df['change'].replace('Ch', 1).replace('No', 0)
df['gender'] = df['gender'].replace('Male', 1).replace('Female', 0)
df['diabetesMed'] = df['diabetesMed'].replace('Yes', 1).replace('No', 0)
df['readmitted'] = df['readmitted'].replace({'<30': 1, '>30': 0, 'NO': 0})

for col in drug_features:
    df[col] = df[col].replace({'No': 0, 'Steady': 1, 'Up': 1, 'Down': 1})

# Replace A1Cresult and max_glu_serum values
df['A1Cresult'] = df['A1Cresult'].replace({'>7': 1, '>8': 1, 'Norm': 0})
df['max_glu_serum'] = df['max_glu_serum'].replace({'>200': 1, '>300': 1, 'Norm': 0})

# Replace real NaN values with -99
df['A1Cresult'] = df['A1Cresult'].fillna(-99)
df['max_glu_serum'] = df['max_glu_serum'].fillna(-99)

print(df[['A1Cresult', 'max_glu_serum']].isnull().sum())

"""Mengategorikan kode kode pada `diag_1`, `diag_2`, dan `diag_3` sesuai dengan tabel jurnal. Kemudian, akan diubah menjadi representasi angka untuk memudahkan model."""

# Define a function to map ICD9 codes to primary diagnosis groups
def map_primary_diagnosis(value):
    if value.startswith(('V', 'E')):  # External causes
        return 'Other'
    if value == '?':  # Missing/invalid values
        return 'Other'

    value = float(value)

    if (390 <= value < 460) or (np.floor(value) == 785):
        return 'Circulatory'
    elif (460 <= value < 520) or (np.floor(value) == 786):
        return 'Respiratory'
    elif (520 <= value < 580) or (np.floor(value) == 787):
        return 'Digestive'
    elif np.floor(value) == 250:
        return 'Diabetes'
    elif 800 <= value < 1000:
        return 'Injury'
    elif 710 <= value < 740:
        return 'Musculoskeletal'
    elif (580 <= value < 630) or (np.floor(value) == 788):
        return 'Genitourinary'
    elif 140 <= value < 240:
        return 'Neoplasms'
    elif 780 <= value < 800:
        return 'Other symptoms and ill-defined conditions'
    elif (240 <= value < 280) and np.floor(value) != 250:
        return 'Endocrine, nutritional, metabolic'
    elif (680 <= value < 710) or (np.floor(value) == 782):
        return 'Skin and subcutaneous'
    elif 1 <= value < 140:
        return 'Infectious and parasitic diseases'
    elif 290 <= value < 320:
        return 'Mental disorders'
    elif 280 <= value < 290:
        return 'Blood and blood-forming organs'
    elif 320 <= value < 360:
        return 'Nervous system'
    elif 630 <= value < 680:
        return 'Pregnancy and childbirth'
    elif 360 <= value < 390:
        return 'Sense organs'
    elif 740 <= value < 760:
        return 'Congenital anomalies'
    else:
        return 'Other'

# Apply the mapping function to diagnostic columns
for diag_col in ['diag_1', 'diag_2', 'diag_3']:
    df[f'group_{diag_col}'] = df[diag_col].apply(map_primary_diagnosis)

# Example usage
print(df[['diag_1', 'group_diag_1']].head())
print(df[['diag_2', 'group_diag_2']].head())
print(df[['diag_3', 'group_diag_3']].head())

# Combine diagnosis group columns into one (optional step for simplicity)
all_diag = pd.concat([df['group_diag_1'], df['group_diag_2'], df['group_diag_3']])

# Apply label encoding
encoder = LabelEncoder()
encoder.fit(all_diag.dropna())  # Drop NaN values before fitting

# Transform each group column
for col in ['group_diag_1', 'group_diag_2', 'group_diag_3']:
    df[f'{col}_encoded'] = encoder.transform(df[col].fillna('Other'))

# Example output
print(df[['group_diag_1', 'group_diag_1_encoded']].head())

"""Jika ditinjau dari id pasien pada `patient_nbr`, terdapat beberapa id yang melakukan readmisi lebih dari sekali. Untuk mengatasi bias output, row yang digunakan hanya readmisi pertama dari setiap pasien. Hasilnya, berkurang sekitar lebih dari 30.000 data"""

df = df.drop_duplicates(subset= ['patient_nbr'], keep = 'first')
df.shape

"""### Feature Engineering

Menggabungkan `change` dan `diabetesMed` menjadi satu fitur untuk menunjukkan apakah ada perubahan pengobatan yang dilakukan untuk pasien yang menerima pengobatan diabetes.
"""

# Create interaction feature: change_diabetesMed
df['change_diabetesMed'] = ((df['change'] == 1) & (df['diabetesMed'] == 1)).astype(int)

# Example explanation:
# - If `change` == 1 and `diabetesMed` == 1, set value to 1; otherwise, 0.
# - This indicates whether the patient receiving diabetes medication had any changes.

"""Menganalisis pola pemanfaatan layanan kesehatan pasien dengan membuat rasio atau tanda yang merangkum interaksi mereka dengan fasilitas layanan kesehatan.

Rasio seperti pasien gawat darurat dengan pasien rawat inap dan pasien rawat jalan dengan pasien rawat inap memberikan wawasan berharga tentang pola pemanfaatan layanan kesehatan pasien. Metrik ini mengungkap tren dalam cara pasien berinteraksi dengan berbagai jenis layanan kesehatan dan dapat menginformasikan alokasi sumber daya, manajemen perawatan, dan pemodelan prediktif.


"""

# Create ratios
df['emergency_to_inpatient'] = (df['number_emergency'] / (df['number_inpatient'] + 1)).fillna(0)
df['outpatient_to_inpatient'] = (df['number_outpatient'] / (df['number_inpatient'] + 1)).fillna(0)

# Create cumulative visit count
df['total_visits'] = df['number_outpatient'] + df['number_emergency'] + df['number_inpatient']

# Flag high-utilization patterns
df['high_utilization'] = (df['total_visits'] > 10).astype(int)

# List of columns to drop
columns_to_drop = ['diag_1', 'diag_2', 'diag_3', 'group_diag_1', 'group_diag_2', 'group_diag_3']

# Create df2 by dropping the specified columns
clean_df = df.drop(columns=columns_to_drop)

clean_df.head(5).T

clean_df.shape

"""Setelah beberapa row dihilangkan dan manipulasi fitur, sekarang fitur bertambah menjadi 57 dengan catatan sudah menghapus beberapa fitur yang tidak diperlukan dan menambah fitur dari hasil feature engineering.

## Step 4: EDA

**Distribusi Readmisi**

Jumlah pasien yang dirawat kembali jauh lebih sedikit dibandingkan dengan pasien yang tidak dirawat kembali.
"""

# Distribution of Readmission
sns.countplot(x='readmitted', data=clean_df).set_title('Distribusi Readmission')
plt.show()

"""Kita buat dataframe baru hanya untuk meninjau korelasi agar setiap parameter merupakan parameter angka. Korelasi yang ditinjau adalah korelasi readmission dan setiap parameter lainnya."""

corr = clean_df.corr()
plt.figure(figsize=(15, 10))
sns.heatmap(corr[['readmitted']].sort_values(by='readmitted', ascending=False), annot=True, cmap='coolwarm')
plt.title("Korelasi Fitur dengan Readmitted")
plt.show()

"""**Gender vs Readmission**"""

sns.barplot(x='gender', y='readmitted', data=clean_df)
plt.title("Readmission Berdasarkan Gender")

"""**Waktu di Rumah Sakit vs Readmission**"""

fig = plt.figure(figsize=(13,7),)
ax=sns.kdeplot(clean_df.loc[(clean_df['readmitted'] == 0),'time_in_hospital'] , color='b',fill=True,label='Not Readmitted')
ax=sns.kdeplot(clean_df.loc[(clean_df['readmitted'] == 1),'time_in_hospital'] , color='r',fill=True, label='Readmitted')
ax.set(xlabel='Time in Hospital', ylabel='Frequency')
plt.title('Readmission Berdasarkan Waktu di Rumah Sakit')

sns.boxplot(x='readmitted', y='time_in_hospital', data=clean_df)
plt.title("Distribusi Time in Hospital Berdasarkan Readmission")

"""**Usia vs Readmission**"""

fig = plt.figure(figsize=(15,10))
sns.countplot(y= clean_df['age'], hue = clean_df['readmitted']).set_title('Readmission Berdasarkan Usia')

"""**Etnis vs Readmission**"""

fig = plt.figure(figsize=(8,8))
sns.countplot(y = clean_df['race'], hue = clean_df['readmitted']).set_title('Readmission Berdasarkan Etnis')

"""**Jumlah Pengobatan yang Digunakan vs Readmission**"""

fig = plt.figure(figsize=(8,8))
sns.barplot(x = clean_df['readmitted'], y = clean_df['num_medications']).set_title("Readmission Berdasarkan Jumlah Pengobatan yang Digunakan")

"""**Pertukaran Pengobatan vs Readmission**"""

fig = plt.figure(figsize=(8,8))
sns.countplot(x = clean_df['change'], hue = clean_df['readmitted']).set_title('Readmission Berdasarkan Jumlah Pertukaran Pengobatan')

"""**Pengobatan Diabetes vs Readmission**"""

fig = plt.figure(figsize=(8,8))
sns.countplot(x = clean_df['diabetesMed'], hue = clean_df['readmitted']).set_title('Readmission Berdasarkan Pengobatan Diabetes')

"""**Total Kunjungan vs Readmission**"""

fig = plt.figure(figsize=(8,8))
sns.barplot( y = clean_df['total_visits'], x = clean_df['readmitted']).set_title('Readmission Berdasarkan Total Kunjungan')

"""**Hasil Tes Serum Glukosa vs Readmission**

Tes Glukosa Serum - Tes glukosa darah digunakan untuk mengetahui apakah kadar gula darah Anda berada dalam kisaran yang sehat. Tes ini sering digunakan untuk membantu mendiagnosis dan memantau diabetes.

- '>200' : 1 = menunjukkan diabetes
- '>300' : 1 = menunjukkan diabetes
- 'Norm' : 0 = Normal
- 'None' : -99 = tes tidak dilakukan
"""

fig = plt.figure(figsize=(8,8))
sns.countplot(y = clean_df['max_glu_serum'], hue = clean_df['readmitted']).set_title('Readmission Berdasarkan Hasil Tes Serum Glukosa')

"""**Hasil A1C vs Readmission**

Tes A1C adalah tes darah yang memberikan informasi tentang kadar glukosa darah rata-rata Anda, yang juga disebut gula darah, selama 3 bulan terakhir.

- '>7' : 1
- '>8' : 1
- Norm : 0 = Normal
- None : -99 = Tes tidak dilakukan
"""

fig = plt.figure(figsize=(8,8))
sns.countplot(y= clean_df['A1Cresult'], hue = clean_df['readmitted']).set_title('Readmission Berdasarkan Hasil Tes A1C')

"""**Jumlah Prosedur Lab vs Readmission**"""

fig = plt.figure(figsize=(15,6),)
ax=sns.kdeplot(clean_df.loc[(clean_df['readmitted'] == 0),'num_lab_procedures'] , color='b',fill=True,label='Not readmitted')
ax=sns.kdeplot(clean_df.loc[(clean_df['readmitted'] == 1),'num_lab_procedures'] , color='r',fill=True, label='readmitted')
ax.set(xlabel='Number of lab procedure', ylabel='Frequency')
plt.title('Readmission Berdasarkan Jumlah Prosedur Lab')

sns.boxplot(x='readmitted', y='num_lab_procedures', data=clean_df)
plt.title("Distribusi Jumlah Prosedur Lab Berdasarkan Readmission")

"""## Step 5: Data Postprocessing"""

# Exclude IDs and already categorical features
num_col = clean_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
num_col = list(set(num_col) - {'encounter_id', 'patient_nbr', 'readmitted'})

print("Numeric columns for statdataframe analysis:")
print(num_col)

# Apply Yeo-Johnson transformation (works for positive and negative values)
pt = PowerTransformer(method='yeo-johnson')
df_transformed = df.copy()
df_transformed[num_col] = pt.fit_transform(df[num_col])

print("Power-transformed numeric features:")
print(df_transformed.head())

# # Data Preprocessing (before feature importance)
# X = clean_df.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])  # Drop target and IDs
# y = clean_df['readmitted']

# # Split the dataset into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# print("Train and Test sets created:")
# print(f"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")

# # Scale the data using StandardScaler
# scaler = StandardScaler()

# # Fit the scaler on training data and transform both train and test
# X_train_scaled = scaler.fit_transform(X_train)
# X_test_scaled = scaler.transform(X_test)

# print("Data has been scaled using StandardScaler.")

# # Train Random Forest Classifier
# rf_model = RandomForestClassifier(random_state=42)
# rf_model.fit(X_train_scaled, y_train)

# # Get feature importance
# importances = rf_model.feature_importances_
# feature_importance_rf = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
# feature_importance_rf = feature_importance_rf.sort_values(by='Importance', ascending=False)

# print("Random Forest Feature Importance:")
# print(feature_importance_rf)

# # Plot Feature Importances
# feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)
# feature_importances.sort_values(ascending=False).plot(kind='bar', figsize=(12, 6), title="Feature Importance")

# # Train Lasso model for feature importance
# lasso = Lasso(alpha=0.01)  # Regularization strength
# lasso.fit(X_train_scaled, y_train)

# # Get feature importance (Lasso coefficients)
# feature_importance_lasso = pd.DataFrame({'Feature': X.columns, 'Importance': lasso.coef_})
# feature_importance_lasso = feature_importance_lasso[feature_importance_lasso['Importance'] != 0]
# feature_importance_lasso = feature_importance_lasso.sort_values(by='Importance', ascending=False)

# print("Lasso Feature Importance (Non-zero Coefficients):")
# print(feature_importance_lasso)

# selected_features = list(set(feature_importance_rf['Feature'].head(10)) | set(feature_importance_lasso['Feature']))
# print("Final Selected Features:")

# for i, feature in enumerate(selected_features):
#     print(f"{i+1}. {feature}")

# # Define function to calculate VIF
# def calculate_vif(X):
#     vif = pd.DataFrame()
#     vif['Feature'] = X.columns
#     vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
#     return vif.sort_values(by="VIF", ascending=False)

# X_selected = clean_df[selected_features]

# # Iteratively drop high VIF features
# while True:
#     vif_scores = calculate_vif(X_selected)
#     print(vif_scores)

#     # Check if the highest VIF is greater than 10
#     if vif_scores['VIF'].iloc[0] > 10:
#         feature_to_drop = vif_scores['Feature'].iloc[0]
#         print(f"Dropping feature: {feature_to_drop}")
#         X_selected = X_selected.drop(columns=[feature_to_drop])
#     else:
#         break

# print("Final Selected Features after VIF correction:")
# print(X_selected.columns)

"""## Step 6: Model Development"""

X = clean_df.drop(columns=['readmitted', 'encounter_id', 'patient_nbr'])  # Drop target and IDs
y = clean_df['readmitted']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Pertama, karena EDA readmission menunjukkan data imbalance, kita lakukan teknik SMOTE untuk menduplikasi data secara sintetis. Data yang diduplikasi hanya digunakan untuk data training pada kondisi permodelan seperti logistic regression. Untuk pohon kita tidak menggunakan SMOTE."""

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to training data
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print("Class distribution after SMOTE:")
print(pd.Series(y_train_balanced).value_counts())

scaler = StandardScaler()

# Fit on training data and transform both train and test sets
X_train_scaled_smote = scaler.fit_transform(X_train_balanced)
X_test_scaled = scaler.transform(X_test)

print("Data scaling completed.")

"""### 1. Logistic Regression"""

lr = LogisticRegression(fit_intercept=True, penalty='l1', solver='liblinear', random_state=42)
lr.fit(X_train_scaled_smote, y_train_balanced)

lr_pred = lr.predict(X_test_scaled)

# Confusion Matrix
conf_matrix_lr = pd.crosstab(pd.Series(y_test, name='Actual'),
                          pd.Series(lr_pred, name='Predict'),
                          margins=True)
print("Confusion Matrix Logistic Regression:\n")
print(conf_matrix_lr)

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix Logistic Regression")
plt.show()

# Accuracy, Precision, Recall
accuracy_lr = accuracy_score(y_test, lr_pred)
precision_lr = precision_score(y_test, lr_pred)
recall_lr = recall_score(y_test, lr_pred)
f1_lr = f1_score(y_test, lr_pred)

print("Accuracy: {:.2f}".format(accuracy_lr))
print("Precision: {:.2f}".format(precision_lr))
print("Recall: {:.2f}".format(recall_lr))
print("F1 Score: {:.2f}".format(f1_lr))

"""### 2. Decision Tree

Ketika menggunakan data yang tidak di SMOTE, kita akan melakukan standardisasi ulang untuk digunakan oleh model lainnya.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()

# Fit on training data and transform both train and test sets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Data scaling completed.")

dt = DecisionTreeClassifier(max_depth=28, criterion = "entropy", min_samples_split=10)
dt.fit(X_train_scaled, y_train)

dt_pred = dt.predict(X_test_scaled)

# Confusion Matrix
conf_matrix_dt = pd.crosstab(pd.Series(y_test, name='Actual'),
                          pd.Series(dt_pred, name='Predict'),
                          margins=True)
print("Confusion Matrix Decision Tree:\n")
print(conf_matrix_dt)

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix Decision Tree")
plt.show()

# Accuracy, Precision, Recall
accuracy_dt = accuracy_score(y_test, dt_pred)
precision_dt = precision_score(y_test, dt_pred)
recall_dt = recall_score(y_test, dt_pred)
f1_dt = f1_score(y_test, dt_pred)

print("Accuracy: {:.2f}".format(accuracy_dt))
print("Precision: {:.2f}".format(precision_dt))
print("Recall: {:.2f}".format(recall_dt))
print("F1 Score: {:.2f}".format(f1_dt))

"""### 3. Random Forest"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()

# Fit on training data and transform both train and test sets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Data scaling completed.")

rf = RandomForestClassifier(n_estimators = 10, max_depth=25, criterion = "gini", min_samples_split=10)
rf.fit(X_train_scaled, y_train)

rf_pred = rf.predict(X_test_scaled)

# Confusion Matrix
conf_matrix_rf = pd.crosstab(pd.Series(y_test, name='Actual'),
                          pd.Series(rf_pred, name='Predict'),
                          margins=True)
print("Confusion Matrix Random Forest:\n")
print(conf_matrix_rf)

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix Random Forest")
plt.show()

# Accuracy, Precision, Recall
accuracy_rf = accuracy_score(y_test, rf_pred)
precision_rf = precision_score(y_test, rf_pred)
recall_rf = recall_score(y_test, rf_pred)
f1_rf = f1_score(y_test, rf_pred)

print("Accuracy: {:.2f}".format(accuracy_rf))
print("Precision: {:.2f}".format(precision_rf))
print("Recall: {:.2f}".format(recall_rf))
print("F1 Score: {:.2f}".format(f1_rf))

"""Untuk dapat menjawab pertanyaan pertama, dapat kita lakukan dengan mencari feature importance"""

# Create list of top most features based on importance
feature_names = X_train.columns
feature_imports = rf.feature_importances_
most_imp_features = pd.DataFrame([f for f in zip(feature_names,feature_imports)], columns=["Feature", "Importance"]).nlargest(10, "Importance")
most_imp_features.sort_values(by="Importance", inplace=True)
plt.figure(figsize=(10,6))
plt.barh(range(len(most_imp_features)), most_imp_features.Importance, align='center', alpha=0.8)
plt.yticks(range(len(most_imp_features)), most_imp_features.Feature, fontsize=14)
plt.xlabel('Importance')
plt.title('Most important features - Random Forest ')
plt.show()

"""### 4. Neural Network"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to training data
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print("Class distribution after SMOTE:")
print(pd.Series(y_train_balanced).value_counts())

scaler = StandardScaler()

# Fit on training data and transform both train and test sets
X_train_scaled = scaler.fit_transform(X_train_balanced)
X_test_scaled = scaler.transform(X_test)

print("Data scaling completed.")

ann_model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Input Layer
    Dropout(0.3),
    Dense(64, activation='relu'),  # Hidden Layer
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # Output Layer (Sigmoid for binary classification)
])

ann_model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = ann_model.fit(X_train_scaled_smote, y_train_balanced,
                    epochs=50,
                    batch_size=32,
                    validation_data=(X_test_scaled, y_test))

ann_pred_prob = ann_model.predict(X_test_scaled).flatten()
ann_pred = np.where(nn_pred_prob > 0.5, 1, 0)

plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('ANN Training & Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

conf_matrix_ann = pd.crosstab(pd.Series(y_test, name='Actual'),
                          pd.Series(ann_pred, name='Predict'),
                          margins=True)
print("Confusion Matrix Neural Network:\n")
print(conf_matrix_ann)

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_ann, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix Neural Network")
plt.show()

# Accuracy, Precision, Recall
accuracy_ann = accuracy_score(y_test, ann_pred)
precision_ann = precision_score(y_test, ann_pred)
recall_ann = recall_score(y_test, ann_pred)
f1_ann = f1_score(y_test, ann_pred)

print("Accuracy: {:.2f}".format(accuracy_ann))
print("Precision: {:.2f}".format(precision_ann))
print("Recall: {:.2f}".format(recall_ann))
print("F1 Score: {:.2f}".format(f1_ann))

"""## Step 7: Model Evaluation"""

# Function to evaluate models
def evaluate_model(model, X_test, y_test, is_ann=False):
    if is_ann:
        y_pred_prob = model.predict(X_test).flatten()
        y_pred = np.where(y_pred_prob > 0.5, 1, 0)
    else:
        y_pred = model.predict(X_test)

    # Print Metrics
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print("-" * 50)
    return y_pred

# Logistic Regression
print("Logistic Regression Results:")
lr_pred = evaluate_model(lr, X_test_scaled, y_test)

# Decision Tree
print("Decision Tree Results:")
dt_pred = evaluate_model(dt, X_test_scaled, y_test)

# Random Forest
print("Random Forest Results:")
rf_pred = evaluate_model(rf, X_test_scaled, y_test)

# ANN
print("ANN Results:")
ann_pred = evaluate_model(ann_model, X_test_scaled, y_test, is_ann=True)

plt.figure(figsize=(14, 7))
ax = plt.subplot(111)

models = ['Logistic Regression', 'Decision Tree', 'Random Forests', 'Neural Network']
values = [accuracy_lr, accuracy_dt, accuracy_rf, accuracy_ann]
model = np.arange(len(models))

plt.bar(model, values, align='center', width = 0.15, alpha=0.7, color = 'red', label= 'accuracy')
plt.xticks(model, models)



ax = plt.subplot(111)

models = ['Logistic Regression', 'Decision Tree', 'Random Forests', 'Neural Network']
values = [precision_lr, precision_dt, precision_rf, precision_ann]
model = np.arange(len(models))

plt.bar(model+0.15, values, align='center', width = 0.15, alpha=0.7, color = 'blue', label = 'precision')
plt.xticks(model, models)



ax = plt.subplot(111)

models = ['Logistic Regression', 'Decision Tree', 'Random Forests', 'Neural Network']
values = [recall_lr, recall_dt, recall_rf, recall_ann]
model = np.arange(len(models))

plt.bar(model+0.3, values, align='center', width = 0.15, alpha=0.7, color = 'green', label = 'recall')
plt.xticks(model, models)



plt.ylabel('Performance Metrics for Different models')
plt.title('Model')

# removing the axis on the top and right of the plot window
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.legend()

plt.show()

"""## Step 8: Model Prediction"""

# Combine predictions into a single DataFrame
results_df = pd.DataFrame({
    'Actual': y_test,
    'Logistic_Regression': lr_pred,
    'Decision_Tree': dt_pred,
    'Random_Forest': rf_pred,
    'ANN': ann_pred
})

print("Model Predictions:")
print(results_df.head(20))